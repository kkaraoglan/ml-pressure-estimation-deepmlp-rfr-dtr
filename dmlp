import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import matplotlib

matplotlib.use('Agg')

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import matplotlib


class RPropMLPEstimator(BaseEstimator, RegressorMixin):
    def __init__(self, n_hidden_layers=5, n_neurons=10, max_iter=100,
                 eta_plus=1.2, eta_minus=0.5, delta_min=1e-6, delta_max=50.0,
                 random_state=42, patience=20, min_delta=1e-4, dropout_rate=0.001):
        self.n_hidden_layers = n_hidden_layers
        self.n_neurons = n_neurons
        self.max_iter = max_iter
        self.eta_plus = eta_plus
        self.eta_minus = eta_minus
        self.delta_min = delta_min
        self.delta_max = delta_max
        self.random_state = random_state
        self.patience = patience
        self.min_delta = min_delta
        self.dropout_rate = dropout_rate
        self.history = {'train_loss': [], 'val_loss': []}
        np.random.seed(random_state)

    def _initialize_weights(self, input_dim):
        self.weights = []
        self.prev_gradients = []
        self.deltas = []

        w = np.random.randn(input_dim, self.n_neurons) * np.sqrt(2.0 / input_dim)
        self.weights.append(w)
        self.prev_gradients.append(np.zeros_like(w))
        self.deltas.append(np.ones_like(w) * 0.1)

        for i in range(self.n_hidden_layers - 1):
            w = np.random.randn(self.n_neurons, self.n_neurons) * np.sqrt(2.0 / self.n_neurons)
            self.weights.append(w)
            self.prev_gradients.append(np.zeros_like(w))
            self.deltas.append(np.ones_like(w) * 0.1)

        w = np.random.randn(self.n_neurons, 1) * np.sqrt(2.0 / self.n_neurons)
        self.weights.append(w)
        self.prev_gradients.append(np.zeros_like(w))
        self.deltas.append(np.ones_like(w) * 0.1)

    def _dropout(self, X, training=True):
        if training and self.dropout_rate > 0:
            mask = np.random.binomial(1, 1 - self.dropout_rate, size=X.shape) / (1 - self.dropout_rate)
            return X * mask
        return X

    def _forward_pass(self, X, training=True):
        self.activations = [X]
        self.z_values = []
        self.dropout_masks = []

        z = np.dot(X, self.weights[0])
        self.z_values.append(z)
        h = np.tanh(z)
        h = self._dropout(h, training)
        self.activations.append(h)

        for i in range(1, self.n_hidden_layers):
            z = np.dot(h, self.weights[i])
            self.z_values.append(z)
            h = np.tanh(z)
            h = self._dropout(h, training)
            self.activations.append(h)

        output = np.dot(h, self.weights[-1])
        self.z_values.append(output)
        self.activations.append(output)

        return output

    def _backward_pass(self, X, y, output):
        m = X.shape[0]
        gradients = []

        error = output - y.reshape(-1, 1)
        grad = np.dot(self.activations[-2].T, error) / m
        gradients.insert(0, grad)

        for i in range(len(self.weights) - 2, -1, -1):
            error = np.dot(error, self.weights[i + 1].T) * (1 - np.tanh(self.z_values[i]) ** 2)
            grad = np.dot(self.activations[i].T, error) / m
            gradients.insert(0, grad)

        return gradients

    def _calculate_loss(self, y_true, y_pred):
        return np.mean(np.square(y_true.reshape(-1, 1) - y_pred))

    def _update_weights(self, gradients):
        for i in range(len(self.weights)):
            sign = np.sign(gradients[i] * self.prev_gradients[i])

            self.deltas[i] = np.where(sign > 0,
                                      np.minimum(self.deltas[i] * self.eta_plus, self.delta_max),
                                      np.where(sign < 0,
                                               np.maximum(self.deltas[i] * self.eta_minus, self.delta_min),
                                               self.deltas[i]))

            update = -np.sign(gradients[i]) * self.deltas[i]
            self.weights[i] += update

            self.prev_gradients[i] = np.where(sign < 0, 0, gradients[i])

    def fit(self, X, y):
        try:
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=self.random_state
            )

            self._initialize_weights(X.shape[1])

            best_loss = float('inf')
            patience_counter = 0
            best_weights = None

            for epoch in range(self.max_iter):
                train_output = self._forward_pass(X_train, training=True)
                gradients = self._backward_pass(X_train, y_train, train_output)
                self._update_weights(gradients)

                train_loss = self._calculate_loss(y_train, train_output)
                val_output = self._forward_pass(X_val, training=False)
                val_loss = self._calculate_loss(y_val, val_output)

                self.history['train_loss'].append(train_loss)
                self.history['val_loss'].append(val_loss)

                if val_loss < best_loss - self.min_delta:
                    best_loss = val_loss
                    patience_counter = 0
                    best_weights = [w.copy() for w in self.weights]
                else:
                    patience_counter += 1

                if patience_counter >= self.patience:
                    if best_weights is not None:
                        self.weights = best_weights
                    break

            return self

        except Exception as e:
            print(f"Error during fitting: {str(e)}")
            raise

    def predict(self, X):
        try:
            return self._forward_pass(X, training=False)
        except Exception as e:
            print(f"Error during prediction: {str(e)}")
            raise

def create_publication_quality_plots(fileName,best_model, target_column, metrics, early_stopping_epoch):
    plt.style.use('default')
    plt.rcParams.update({
        'figure.facecolor': 'white',
        'axes.facecolor': 'white',
        'axes.edgecolor': 'black',
        'axes.labelcolor': 'black',
        'axes.grid': True,
        'grid.color': 'gray',
        'grid.linestyle': '--',
        'grid.alpha': 0.3,
        'lines.linewidth': 2.5,
        'font.family': 'sans-serif',
        'font.size': 14,  # Genel font boyutu artırıldı (12 -> 14)
        'text.color': 'black',
        'xtick.color': 'black',
        'ytick.color': 'black',
        'xtick.direction': 'out',
        'ytick.direction': 'out',
        'axes.spines.top': True,
        'axes.spines.right': True
    })

    safe_target = target_column.replace(" ", "_").replace("(", "").replace(")", "")

    fig, ax = plt.subplots(figsize=(12, 10), dpi=300)

    ax.plot(best_model.history['train_loss'],
            label='Deep-MLP Training MSE',
            color='#2E86C1',  # Daha koyu mavi
            linewidth=3.5,  # Çizgi kalınlığı artırıldı
            alpha=0.9,  # Opaklık artırıldı
            zorder=3)

    ax.plot(best_model.history['val_loss'],
            label='Deep-MLP Test MSE',
            color='#E74C3C',  # Daha koyu kırmızı
            linewidth=3.5,  # Çizgi kalınlığı artırıldı
            alpha=0.9,  # Opaklık artırıldı
            zorder=3)

    ax.axvline(x=early_stopping_epoch,
               color='#009988',
               linestyle='--',
               linewidth=2.5,  # Increased line width
               alpha=0.8,  # Increased opacity
               label=f'Early Stopping Point\n(Epoch {early_stopping_epoch})',
               zorder=2)

    ax.scatter(early_stopping_epoch,
               best_model.history['val_loss'][early_stopping_epoch],
               color='#009988',
               s=200,  # Nokta boyutu artırıldı
               zorder=4,
               edgecolor='white',
               linewidth=2.5,  # Kenar çizgisi kalınlığı artırıldı
               marker='o')  # Belirgin marker

    ax.set_xlabel('Training Epochs', fontsize=17, fontweight='bold')
    ax.set_ylabel('Mean Squared Error (MSE)', fontsize=17, fontweight='bold')

    title = f'Deep-MLP Training Convergence Analysis for {fileName}\n{target_column}'
    subtitle = (f'$R^2_{{\mathrm{{train}}}}$ = {metrics["train_r2"]:.4f}, '
                f'$R^2_{{\mathrm{{test}}}}$ = {metrics["test_r2"]:.4f}\n'
                f'$\mathrm{{RMSE}}_{{train}}$ = {metrics["train_rmse"]:.2e}, '
                f'$\mathrm{{RMSE}}_{{test}}$ = {metrics["test_rmse"]:.2e}')

    ax.set_title(title + '\n' + subtitle,
                 fontsize=19,
                 fontweight='bold',
                 pad=20)

    ax.tick_params(axis='both',
                   labelsize=14,
                   width=2,  # Tick çizgilerinin kalınlığı
                   length=6,  # Tick çizgilerinin uzunluğu
                   labelcolor='black',
                   colors='black')

    ax.grid(True,
            linestyle='--',
            alpha=0.4,  # Izgara opaklığı artırıldı
            linewidth=1.2,  # Izgara çizgi kalınlığı
            color='gray',
            zorder=1)

    ax.legend(loc='upper right',
              fontsize=14,
              frameon=True,
              fancybox=True,
              framealpha=0.95,
              edgecolor='gray',
              shadow=True,
              borderpad=1,  # İç padding
              labelspacing=1.2,  # Etiketler arası boşluk
              handlelength=3,  # Legend çizgi uzunluğu
              handletextpad=0.8)  # Çizgi ve metin arası boşluk

    for spine in ax.spines.values():
        spine.set_linewidth(1.5)

    plt.tight_layout()
    plt.savefig(f'{fileName}_Deep-MLP_training_history_{safe_target}.png',
                dpi=300,
                bbox_inches='tight',
                facecolor='white',
                edgecolor='none')
    plt.close()

    fig, ax = plt.subplots(figsize=(12, 10), dpi=300)

    ax.scatter(metrics['y_train_true'],
               metrics['y_train_pred'],
               c='green',
               alpha=0.7,
               s=120,  # Nokta boyutu
               marker='o',
               label='Training Data',
               zorder=3)

    ax.scatter(metrics['y_test_true'],
               metrics['y_test_pred'],
               c='blue',
               alpha=0.7,
               s=120,  # Nokta boyutu
               marker='x',
               label='Test Data',
               zorder=3)

    max_val = max(np.max(metrics['y_train_true']), np.max(metrics['y_train_pred']),
                  np.max(metrics['y_test_true']), np.max(metrics['y_test_pred']))
    min_val = min(np.min(metrics['y_train_true']), np.min(metrics['y_train_pred']),
                  np.min(metrics['y_test_true']), np.min(metrics['y_test_pred']))
    padding = (max_val - min_val) * 0.1

    ax.plot([min_val - padding, max_val + padding],
            [min_val - padding, max_val + padding],
            'k--',
            linewidth=2.0,
            label='Perfect Prediction',
            zorder=2)

    ax.set_xlabel('Experimental Values', fontsize=17, fontweight='bold')
    ax.set_ylabel('Predicted Values', fontsize=17, fontweight='bold')

    ax.set_title(f'Experimental - Predicted Values for {fileName.split(".")[0]} using Deep-MLP Model',
                 fontsize=19,
                 fontweight='bold',
                 pad=20)

    ax.tick_params(axis='both', labelsize=14)

    metrics_text = (
        f'R² = {metrics["test_r2"]:.4f}\n'
        f'RMSE = {metrics["test_rmse"]:.4f}\n'
        f'MAE = {metrics["test_mae"]:.4f}\n'
        f'MAPE = {metrics["test_mape"]:.2f}%'
    )
    ax.text(0.95, 0.05, metrics_text,
            transform=ax.transAxes,
            verticalalignment='bottom',
            horizontalalignment='right',
            fontsize=17,
            fontweight='bold',
            fontname='Arial',
            bbox=dict(boxstyle='round',
                      facecolor='white',
                      alpha=0.8,
                      pad=1.0))  # Padding artırıldı

    ax.grid(True, linestyle='--', alpha=0.7, zorder=1)
    ax.legend(fontsize=14,
              loc='upper left',
              frameon=True,  # Frame görünürlüğü açıldı
              framealpha=0.9,
              edgecolor='gray',
              fancybox=True,
              shadow=True)

    ax.set_xlim([min_val - padding, max_val + padding])
    ax.set_ylim([min_val - padding, max_val + padding])
    ax.set_aspect('equal')

    plt.tight_layout()
    plt.savefig(f'{fileName}_Deep-MLP_train_test_prediction_performance_{safe_target}.png',
                dpi=300,
                bbox_inches='tight',
                facecolor='white',
                edgecolor='none')
    plt.close()

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8), dpi=300)  # Figure boyutu artırıldı

    train_errors = metrics['y_train_pred'].ravel() - metrics['y_train_true'].ravel()
    test_errors = metrics['y_test_pred'].ravel() - metrics['y_test_true'].ravel()

    bp = ax1.boxplot([train_errors, test_errors],
                     labels=['Training', 'Test'],
                     patch_artist=True)

    bp['boxes'][0].set_facecolor('#0077BB')
    bp['boxes'][1].set_facecolor('#EE3377')

    ax1.set_ylabel('Prediction Error', fontsize=17, fontweight='bold')
    ax1.set_title(f'Deep-MLP Error Distribution Analysis for {fileName.split(".")[0]}\n' + target_column,
                  fontsize=19,
                  fontweight='bold',
                  pad=20)
    ax1.tick_params(axis='both', labelsize=14)
    ax1.grid(True, linestyle='--', alpha=0.3)
    ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)

    from scipy.stats import gaussian_kde

    def plot_density(data, color, label, ax):
        density = gaussian_kde(data)
        xs = np.linspace(min(data), max(data), 200)
        ax.plot(xs, density(xs), color=color, linewidth=2.5, label=label)
        ax.fill_between(xs, density(xs), alpha=0.2, color=color)

    plot_density(train_errors, '#0077BB', 'Training Error', ax2)
    plot_density(test_errors, '#EE3377', 'Test Error', ax2)

    ax2.set_xlabel('Prediction Error', fontsize=17, fontweight='bold')
    ax2.set_ylabel('Density', fontsize=17, fontweight='bold')
    ax2.set_title(f'Deep-MLP Error Distribution Density for {fileName}\n' + target_column,
                  fontsize=19,
                  fontweight='bold',
                  pad=20)
    ax2.tick_params(axis='both', labelsize=14)
    ax2.grid(True, linestyle='--', alpha=0.3)
    ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)
    ax2.legend(fontsize=14)

    plt.tight_layout()
    plt.savefig(f'{fileName}_Deep-MLP_error_analysis_{safe_target}.png',
                dpi=300,
                bbox_inches='tight',
                facecolor='white',
                edgecolor='none')
    plt.close()

    print(f"Plots successfully saved for {fileName}: {safe_target}")

def process_dataset_with_gridsearch(file_name, target_column):
    try:
        print(f"\nLoading dataset from {file_name}...")
        df = pd.read_excel(file_name)
        if df.empty:
            raise ValueError("Dataset is empty")

        feature_columns = [col for col in df.columns if col != target_column]
        if not feature_columns:
            raise ValueError("No feature columns found")

        print(f"{file_name}_Features: {feature_columns}")
        print(f"{file_name}_Target: {target_column}")

        X = df[feature_columns].values
        y = df[target_column].values

        if np.isnan(X).any() or np.isnan(y).any():
            raise ValueError("Dataset contains NaN values")

        scaler_X = MinMaxScaler()
        scaler_y = MinMaxScaler()

        X_normalized = scaler_X.fit_transform(X)
        y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()

        indices = np.arange(len(X))
        X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(
            X_normalized, y_normalized, indices,
            test_size=0.2, random_state=42, shuffle=True
        )

        original_df = df.copy()
        original_df.reset_index(inplace=True, drop=True)
        original_df['Original_Index'] = np.arange(len(df))

        param_grid = {
            'n_hidden_layers': [3, 5],
            'n_neurons': [15, 20],
            'max_iter': [300],
            'patience': [20, 30],
            'dropout_rate': [0.0],
            'eta_plus': [1.1, 1.2],
            'eta_minus': [0.3, 0.4],
            'delta_min': [1e-7, 1e-6],
            'delta_max': [10.0, 20.0, 30.0],
            'random_state': [42],
            'min_delta': [1e-5, 1e-4]
        }

        print(f"\n{file_name}_Grid Search Parameters:")
        for param, values in param_grid.items():
            print(f"{param}: {values}")

        cv_values = [5]  # Örneğin, 3, 5 ve 7 katlı çapraz doğrulama

        best_score = float('-inf')
        best_cv = None
        best_params = None
        best_model = None

        for cv_value in cv_values:
            print(f"\nTrying CV={cv_value}...")

            # GridSearchCV'yi ayarla
            grid_search = GridSearchCV(
                estimator=RPropMLPEstimator(),
                param_grid=param_grid,
                cv=cv_value,  # Her bir cv değeri için GridSearchCV
                scoring='neg_mean_squared_error',
                n_jobs=1,  # Paralel işlem sayısı
                verbose=1,  # Detaylı çıktı
                refit=True  # En iyi parametrelerle modeli yeniden eğit
            )

            grid_search.fit(X_normalized, y_normalized)

            if grid_search.best_score_ > best_score:
                best_score = grid_search.best_score_
                best_cv = cv_value
                best_params = grid_search.best_params_
                best_model = grid_search.best_estimator_

        print(f"\nBest CV value: {best_cv}")
        print(f"Best parameters: {best_params}")
        print(f"Best cross-validation score: {-best_score}")  # Negatif MSE'yi pozitife çevir

        early_stopping_epoch = len(best_model.history['train_loss']) - best_params['patience']

        y_train_pred = best_model.predict(X_train)
        y_train_true = y_train.reshape(-1, 1)

        y_test_pred = best_model.predict(X_test)
        y_test_true = y_test.reshape(-1, 1)

        y_train_pred_original = scaler_y.inverse_transform(y_train_pred)
        y_train_true_original = scaler_y.inverse_transform(y_train_true)
        y_test_pred_original = scaler_y.inverse_transform(y_test_pred)
        y_test_true_original = scaler_y.inverse_transform(y_test_true)

        metrics = {
            'train_r2': r2_score(y_train_true_original, y_train_pred_original),
            'test_r2': r2_score(y_test_true_original, y_test_pred_original),
            'train_rmse': np.sqrt(mean_squared_error(y_train_true_original, y_train_pred_original)),
            'test_rmse': np.sqrt(mean_squared_error(y_test_true_original, y_test_pred_original)),
            'train_mae': mean_absolute_error(y_train_true_original, y_train_pred_original),
            'test_mae': mean_absolute_error(y_test_true_original, y_test_pred_original),
            'train_mape': np.mean(
                np.abs((y_train_true_original - y_train_pred_original) / y_train_true_original)) * 100,
            'test_mape': np.mean(np.abs((y_test_true_original - y_test_pred_original) / y_test_true_original)) * 100,
            'y_train_true': y_train_true_original,
            'y_train_pred': y_train_pred_original,
            'y_test_true': y_test_true_original,
            'y_test_pred': y_test_pred_original,
            'best_cv': best_cv
        }

        train_results = pd.DataFrame({
            'Original_Index': train_indices,
            'Set': ['Train'] * len(y_train_true_original),
            'Actual_Values': y_train_true_original.ravel(),
            'Predicted_Values': y_train_pred_original.ravel(),
            'Error': y_train_pred_original.ravel() - y_train_true_original.ravel(),
            'Absolute_Error': np.abs(y_train_pred_original.ravel() - y_train_true_original.ravel()),
            'Percentage_Error': np.abs(
                (y_train_pred_original.ravel() - y_train_true_original.ravel()) / y_train_true_original.ravel()) * 100
        })

        for col in feature_columns:
            train_results[f'Original_{col}'] = original_df.loc[train_indices, col].values

        test_results = pd.DataFrame({
            'Original_Index': test_indices,
            'Set': ['Test'] * len(y_test_true_original),
            'Actual_Values': y_test_true_original.ravel(),
            'Predicted_Values': y_test_pred_original.ravel(),
            'Error': y_test_pred_original.ravel() - y_test_true_original.ravel(),
            'Absolute_Error': np.abs(y_test_pred_original.ravel() - y_test_true_original.ravel()),
            'Percentage_Error': np.abs(
                (y_test_pred_original.ravel() - y_test_true_original.ravel()) / y_test_true_original.ravel()) * 100
        })

        for col in feature_columns:
            test_results[f'Original_{col}'] = original_df.loc[test_indices, col].values

        all_results = pd.concat([train_results, test_results])
        all_results = all_results.sort_values('Original_Index').set_index('Original_Index')


        safe_target = target_column.replace(" ", "_").replace("(", "").replace(")", "")
        results_filename = f'Deep-MLP_Index_{filename}prediction_results_{safe_target}.xlsx'
        try:
            with pd.ExcelWriter(results_filename) as writer:
                train_results.to_excel(writer, sheet_name='Training Results', index=True)

                test_results.to_excel(writer, sheet_name='Test Results', index=True)

                all_results.to_excel(writer, sheet_name='All Results', index=True)

                original_df.to_excel(writer, sheet_name='Original Dataset', index=True)

                cv_results = pd.DataFrame(grid_search.cv_results_)
                cv_results.to_excel(writer, sheet_name='CV Results', index=True)

                performance_metrics = pd.DataFrame({
                    'Metric': ['R2 Score', 'RMSE', 'MAE', 'MAPE'],
                    'Training': [metrics['train_r2'], metrics['train_rmse'],
                                 metrics['train_mae'], metrics['train_mape']],
                    'Testing': [metrics['test_r2'], metrics['test_rmse'],
                                metrics['test_mae'], metrics['test_mape']]
                })
                performance_metrics.to_excel(writer, sheet_name='Performance Metrics', index=False)

        except Exception as e:
            print(f"Warning: Could not save to Excel {file_name}: {str(e)}")

        create_publication_quality_plots(
            fileName=file_name,
            best_model=best_model,
            target_column=target_column,
            metrics=metrics,
            early_stopping_epoch=early_stopping_epoch
        )

        return metrics, best_params

    except Exception as e:
        print(f"Error processing dataset {target_column}: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        datasets = [
            {
                'file': 'DF.xlsx',
                'target': '40NM'
            },
            {
                'file': 'WC030.xlsx',
                'target': '40NM'
            },
            {
                'file': 'WCO30F5.xlsx',
                'target': '40NM'
            },
            {
                'file': 'WCO30F10.xlsx',
                'target': '40NM'
            },
            {
                'file': 'WCO50.xlsx',
                'target': '40NM'
            },
            {
                'file': 'WCO50F5.xlsx',
                'target': '40NM'
            },
            {
                'file': 'WCO50F10.xlsx',
                'target': '40NM'
            }
        ]

        all_results = {}
        for dataset in datasets:
            filename = dataset.get('file')
            print(f"\n {filename}_Processing {dataset['target']}...")
            metrics, best_params = process_dataset_with_gridsearch(
                dataset['file'],
                dataset['target']
            )

            all_results[dataset['target']] = {
                'metrics': metrics,
                'best_params': best_params
            }

            print(f"\n{filename}_Results for {dataset['target']}:")
            print(f"{filename}_Training Set Performance:")
            print(f"{filename}_R² Score: {metrics['train_r2']:.4f}")
            print(f"{filename}_RMSE: {metrics['train_rmse']:.8f}")
            print(f"{filename}_MAE: {metrics['train_mae']:.8f}")
            print(f"{filename}_MAPE: {metrics['train_mape']:.4f}%")

            print(f"\n{filename}_Test Set Performance:")
            print(f"{filename}_R² Score: {metrics['test_r2']:.4f}")
            print(f"{filename}_RMSE: {metrics['test_rmse']:.8f}")
            print(f"{filename}_MAE: {metrics['test_mae']:.8f}")
            print(f"{filename}_MAPE: {metrics['test_mape']:.4f}%")

            print(f"\n{filename}_Best Parameters:", best_params)

        metrics_df = pd.DataFrame()
        params_df = pd.DataFrame()

        for target, results in all_results.items():
            metrics_data = pd.DataFrame({
                'Metric': [
                    'Train R²', 'Test R²',
                    'Train RMSE', 'Test RMSE',
                    'Train MAE', 'Test MAE',
                    'Train MAPE', 'Test MAPE'
                ],
                target: [
                    results['metrics']['train_r2'],
                    results['metrics']['test_r2'],
                    results['metrics']['train_rmse'],
                    results['metrics']['test_rmse'],
                    results['metrics']['train_mae'],
                    results['metrics']['test_mae'],
                    results['metrics']['train_mape'],
                    results['metrics']['test_mape']
                ]
            }).set_index('Metric')

            metrics_df = pd.concat([metrics_df, metrics_df], axis=1)

            # Parametreleri düzenle
            params_data = pd.DataFrame({
                'Parameter': list(results['best_params'].keys()),
                target: list(results['best_params'].values())
            }).set_index('Parameter')

            params_df = pd.concat([params_df, params_data], axis=1)

        with pd.ExcelWriter(f'Deep-MLP{filename}_final_results_summary.xlsx') as writer:
            metrics_df.to_excel(writer, sheet_name='All Metrics')
            params_df.to_excel(writer, sheet_name='Best Parameters')

        print(f"\n{filename}_Processing complete. Results have been saved to:")
        print(f"{filename}_1. Individual prediction results: prediction_results_*.xlsx")
        print(f"{filename}_2. Summary results: final_results_summary.xlsx")

    except Exception as e:
        print(f"Error in main execution: {str(e)}")

